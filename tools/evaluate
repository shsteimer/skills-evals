#!/usr/bin/env node

/**
 * Evaluation Script for Agent Skills Tests
 *
 * Usage:
 *   ./tools/evaluate <output-dir> [options]
 *
 * Arguments:
 *   <output-dir>          Path to test results directory
 *
 * Options:
 *   --eval-agent <agent>  Agent to use for dynamic evaluation (default: claude-code)
 *   --skip-dynamic       Only run static evaluation criteria (faster)
 *   --help                Show this help message
 */

import { readFileSync, writeFileSync, existsSync, readdirSync, statSync } from 'fs';
import { join, dirname, basename } from 'path';
import { fileURLToPath } from 'url';
import { execSync, spawnSync } from 'child_process';
import { parse as parseYaml } from 'yaml';

const __filename = fileURLToPath(import.meta.url);
const __dirname = dirname(__filename);
const PROJECT_ROOT = join(__dirname, '..');

/**
 * Parse command line arguments
 */
function parseArgs() {
  const args = process.argv.slice(2);

  if (args.includes('--help') || args.length === 0) {
    showHelp();
    process.exit(0);
  }

  const options = {
    outputDir: null,
    evalAgent: 'claude-code',  // Default evaluator
    skipNonDeterministic: false
  };

  let i = 0;
  while (i < args.length) {
    const arg = args[i];
    const next = args[i + 1];

    switch (arg) {
      case '--eval-agent':
        options.evalAgent = next || 'claude-code';
        i++;
        break;
      case '--skip-dynamic':
      case '--skip-flexible': // Backward compatibility
        options.skipNonDeterministic = true;
        break;
      default:
        if (!options.outputDir && !arg.startsWith('--')) {
          options.outputDir = arg;
        }
        break;
    }
    i++;
  }

  if (!options.outputDir) {
    console.error('Error: Output directory is required');
    showHelp();
    process.exit(1);
  }

  if (!existsSync(options.outputDir)) {
    console.error(`Error: Output directory does not exist: ${options.outputDir}`);
    process.exit(1);
  }

  return options;
}

/**
 * Show help message
 */
function showHelp() {
  console.log(`
Evaluation Script for Agent Skills Tests

Usage:
  ./tools/evaluate <output-dir> [options]

Arguments:
  <output-dir>          Path to test results directory

Options:
  --eval-agent <agent>  Agent to use for dynamic evaluation (default: claude-code)
  --skip-dynamic  Only run static evaluation criteria (faster)
  --help                Show this help message

Examples:
  # Evaluate all tasks and agents from a run
  ./tools/evaluate evaluations/2025-01-14T10:00:00Z

  # Evaluate specific task across all agents
  ./tools/evaluate evaluations/2025-01-14T10:00:00Z/create-simple-block

  # Evaluate specific agent for specific task
  ./tools/evaluate evaluations/2025-01-14T10:00:00Z/create-simple-block/claude-code

  # Skip dynamic evaluation (static only, faster)
  ./tools/evaluate evaluations/2025-01-14T10:00:00Z --skip-dynamic

  # Use specific agent for dynamic evaluation
  ./tools/evaluate evaluations/2025-01-14T10:00:00Z --eval-agent cursor-cli
`);
}

/**
 * Load task definition from task.yaml
 */
function loadTestDefinition(outputDir) {
  // New structure: evaluations/{timestamp}/{task-name}/{agent}/
  // We need to find the task.yaml by searching tasks/ directory for matching task name

  const parts = outputDir.split('/').filter(p => p);
  const evaluationsIndex = parts.indexOf('evaluations');

  if (evaluationsIndex === -1) {
    throw new Error('Output directory must be under evaluations/');
  }

  // Extract task name from path
  // Structure: evaluations/{timestamp}/{task-name}[/{agent}]
  const taskNameIndex = evaluationsIndex + 2;
  if (taskNameIndex >= parts.length) {
    throw new Error('Invalid output directory structure');
  }

  const taskDirName = parts[taskNameIndex];

  // Search for task.yaml file with matching sanitized name
  const taskYamlPath = findTaskYamlByName(taskDirName);

  if (!taskYamlPath) {
    throw new Error(`Task definition not found for: ${taskDirName}`);
  }

  const content = readFileSync(taskYamlPath, 'utf8');
  return parseYaml(content);
}

/**
 * Find task.yaml file by searching for task with matching sanitized name
 */
function findTaskYamlByName(taskDirName) {
  const tasksDir = join(PROJECT_ROOT, 'tasks');

  // Recursively search for all task.yaml files
  function findTaskYamls(dir) {
    const results = [];
    const entries = readdirSync(dir);

    for (const entry of entries) {
      const fullPath = join(dir, entry);
      const stat = statSync(fullPath);

      if (stat.isDirectory()) {
        results.push(...findTaskYamls(fullPath));
      } else if (entry === 'task.yaml') {
        results.push(fullPath);
      }
    }

    return results;
  }

  // Find all task.yaml files
  const allTaskYamls = findTaskYamls(tasksDir);

  // Check each one to see if the sanitized name matches
  for (const yamlPath of allTaskYamls) {
    try {
      const content = readFileSync(yamlPath, 'utf8');
      const task = parseYaml(content);
      const sanitized = sanitizeTestName(task.name);

      if (sanitized === taskDirName) {
        return yamlPath;
      }
    } catch (e) {
      // Skip invalid yaml files
    }
  }

  return null;
}

/**
 * Sanitize test name for use as directory name (same logic as run_tasks)
 */
function sanitizeTestName(name) {
  return name
    .toLowerCase()
    .replace(/[^a-z0-9]+/g, '-')
    .replace(/^-|-$/g, '')
    .substring(0, 50);
}

/**
 * Detect path type based on structure
 * Returns: 'timestamp', 'task', or 'agent'
 */
function detectPathType(outputDir) {
  const parts = outputDir.split('/').filter(p => p);
  const evaluationsIndex = parts.indexOf('evaluations');

  if (evaluationsIndex === -1) {
    return 'unknown';
  }

  // Count parts after 'evaluations'
  const partsAfterEvaluations = parts.length - evaluationsIndex - 1;

  if (partsAfterEvaluations === 1) {
    // evaluations/{timestamp}
    return 'timestamp';
  } else if (partsAfterEvaluations === 2) {
    // evaluations/{timestamp}/{task-name}
    return 'task';
  } else if (partsAfterEvaluations === 3) {
    // evaluations/{timestamp}/{task-name}/{agent}
    return 'agent';
  }

  return 'unknown';
}

/**
 * Get all task directories in a timestamp directory
 */
function getTaskDirectories(timestampDir) {
  if (!existsSync(timestampDir)) {
    return [];
  }

  const entries = readdirSync(timestampDir);
  const taskDirs = [];

  for (const entry of entries) {
    const fullPath = join(timestampDir, entry);
    const stat = statSync(fullPath);

    if (stat.isDirectory()) {
      taskDirs.push(fullPath);
    }
  }

  return taskDirs;
}

/**
 * Get all agent directories in a task directory
 */
function getAgentDirectoriesForTask(taskDir) {
  if (!existsSync(taskDir)) {
    return [];
  }

  const entries = readdirSync(taskDir);
  const agentDirs = [];

  for (const entry of entries) {
    const fullPath = join(taskDir, entry);
    const stat = statSync(fullPath);

    if (stat.isDirectory()) {
      agentDirs.push(fullPath);
    }
  }

  return agentDirs;
}

/**
 * Auto-select an eval agent different from the agent(s) being tested
 */
function autoSelectEvalAgent(outputDir) {
  const availableAgents = ['claude-code', 'cursor-cli', 'codex-cli'];
  const pathType = detectPathType(outputDir);

  let testedAgents = [];

  if (pathType === 'timestamp') {
    // Get all agents that were tested
    try {
      const agentDirs = getAgentDirectories(outputDir);
      testedAgents = agentDirs.map(dir => basename(dir));
    } catch (e) {
      // If we can't read directories, fall back to claude-code
      return 'claude-code';
    }
  } else {
    // Single agent directory
    const pathParts = outputDir.split('/');
    const agent = pathParts[pathParts.length - 1];
    testedAgents = [agent];
  }

  // Find an agent that wasn't tested
  const untested = availableAgents.filter(a => !testedAgents.includes(a));

  if (untested.length > 0) {
    // Use the first untested agent
    return untested[0];
  }

  // All agents were tested, use claude-code as default
  return 'claude-code';
}

/**
 * Get all agent directories in a timestamp directory
 */
function getAgentDirectories(timestampDir) {
  try {
    const entries = readdirSync(timestampDir);
    return entries.filter(entry => {
      const fullPath = join(timestampDir, entry);
      const stat = statSync(fullPath);
      return stat.isDirectory();
    }).map(dir => join(timestampDir, dir));
  } catch (error) {
    throw new Error(`Failed to read agent directories: ${error.message}`);
  }
}

/**
 * Run static evaluation criteria (required)
 */
async function runDeterministicChecks(outputDir, testDef) {
  console.log('\n=== Running Deterministic Checks ===\n');

  const results = {
    passed: true,
    failures: [],
    checks: {}
  };

  const checks = testDef.static_criteria || {};

  // Check if required files exist (check in git diff)
  if (checks.files_exist) {
    console.log('Checking required files...');

    // Read the code diff to see what files were created/modified
    const diffPath = join(outputDir, 'code-diff.patch');
    let diffContent = '';
    if (existsSync(diffPath)) {
      diffContent = readFileSync(diffPath, 'utf8');
    }

    for (const file of checks.files_exist) {
      // Check if file appears in the diff (as a new or modified file)
      const fileInDiff = diffContent.includes(`+++ b/${file}`) || diffContent.includes(`diff --git a/${file}`);
      results.checks[`file_exists:${file}`] = fileInDiff;

      if (!fileInDiff) {
        results.passed = false;
        results.failures.push(`Required file does not exist: ${file}`);
        console.log(`  ✗ ${file} (MISSING)`);
      } else {
        console.log(`  ✓ ${file}`);
      }
    }
  }

  // Check if files should NOT exist
  if (checks.files_not_exist) {
    console.log('\nChecking files that should not exist...');

    // Read the code diff to see what files were created/modified
    const diffPath = join(outputDir, 'code-diff.patch');
    let diffContent = '';
    if (existsSync(diffPath)) {
      diffContent = readFileSync(diffPath, 'utf8');
    }

    for (const file of checks.files_not_exist) {
      // Check if file appears in the diff
      const fileInDiff = diffContent.includes(`+++ b/${file}`) || diffContent.includes(`diff --git a/${file}`);
      results.checks[`file_not_exists:${file}`] = !fileInDiff;

      if (fileInDiff) {
        results.passed = false;
        results.failures.push(`File should not exist: ${file}`);
        console.log(`  ✗ ${file} (EXISTS)`);
      } else {
        console.log(`  ✓ ${file} (correctly absent)`);
      }
    }
  }

  // Check linting results if required
  if (checks.lint_passes) {
    console.log('\nChecking linting results...');
    const lintResultPath = join(outputDir, 'lint-result.json');

    if (existsSync(lintResultPath)) {
      try {
        const lintResult = JSON.parse(readFileSync(lintResultPath, 'utf8'));
        results.checks['lint_passes'] = lintResult.passed;

        if (lintResult.passed) {
          console.log('  ✓ Linting passed');
        } else {
          results.passed = false;
          results.failures.push('Linting failed');
          console.log('  ✗ Linting failed');
          // Show first few lines of lint output
          if (lintResult.output) {
            const lines = lintResult.output.split('\n').slice(0, 10);
            lines.forEach(line => console.log(`    ${line}`));
            if (lintResult.output.split('\n').length > 10) {
              console.log('    ... (see lint-output.txt for full output)');
            }
          }
        }
      } catch (e) {
        console.log(`  ⚠ Could not parse lint results: ${e.message}`);
        results.checks['lint_passes'] = false;
      }
    } else {
      console.log('  ⚠ Lint results not found (linting may not have run)');
      results.checks['lint_passes'] = false;
    }
  }

  // Check for forbidden patterns
  if (checks.forbidden_patterns) {
    console.log('\nChecking for forbidden patterns...');

    // Read the code diff
    const diffPath = join(outputDir, 'code-diff.patch');
    let diffContent = '';
    if (existsSync(diffPath)) {
      diffContent = readFileSync(diffPath, 'utf8');
    }

    for (const check of checks.forbidden_patterns) {
      const pattern = check.pattern;
      const inFiles = check.in_files || ['**/*'];

      // Search for pattern in added lines (lines starting with +) in the diff
      let found = false;
      const foundIn = [];

      // Split diff into file sections
      const fileSections = diffContent.split(/^diff --git /m).slice(1);

      for (const section of fileSections) {
        const fileMatch = section.match(/^a\/(.+?) b\/(.+?)$/m);
        if (!fileMatch) continue;

        const fileName = fileMatch[2];

        // Check if file matches any of the globs
        const matchesGlob = inFiles.some(glob => {
          if (glob === '**/*') return true;
          // Simple glob matching (could be improved with a proper glob library)
          const globRegex = new RegExp('^' + glob.replace(/\*/g, '.*').replace(/\?/g, '.') + '$');
          return globRegex.test(fileName);
        });

        if (!matchesGlob) continue;

        // Extract added lines (lines starting with +, but not +++)
        const lines = section.split('\n');
        for (const line of lines) {
          if (line.startsWith('+') && !line.startsWith('+++')) {
            // Check if pattern exists in this added line
            const regex = new RegExp(pattern);
            if (regex.test(line)) {
              found = true;
              if (!foundIn.includes(fileName)) {
                foundIn.push(fileName);
              }
            }
          }
        }
      }

      if (found) {
        results.passed = false;
        results.failures.push(`Forbidden pattern found: "${pattern}" in ${foundIn.join(', ')}`);
        results.checks[`forbidden_pattern:${pattern}`] = false;
        console.log(`  ✗ Pattern "${pattern}" found in:`);
        foundIn.forEach(f => console.log(`    - ${f}`));
      } else {
        results.checks[`forbidden_pattern:${pattern}`] = true;
        console.log(`  ✓ Pattern "${pattern}" not found`);
      }
    }
  }

  // Check for required patterns
  if (checks.required_patterns) {
    console.log('\nChecking for required patterns...');

    // Read the code diff
    const diffPath = join(outputDir, 'code-diff.patch');
    let diffContent = '';
    if (existsSync(diffPath)) {
      diffContent = readFileSync(diffPath, 'utf8');
    }

    for (const check of checks.required_patterns) {
      const pattern = check.pattern;
      const inFiles = check.in_files || ['**/*'];

      // Search for pattern in added lines
      let found = false;
      const foundIn = [];

      const fileSections = diffContent.split(/^diff --git /m).slice(1);

      for (const section of fileSections) {
        const fileMatch = section.match(/^a\/(.+?) b\/(.+?)$/m);
        if (!fileMatch) continue;

        const fileName = fileMatch[2];

        const matchesGlob = inFiles.some(glob => {
          if (glob === '**/*') return true;
          const globRegex = new RegExp('^' + glob.replace(/\*/g, '.*').replace(/\?/g, '.') + '$');
          return globRegex.test(fileName);
        });

        if (!matchesGlob) continue;

        const lines = section.split('\n');
        for (const line of lines) {
          if (line.startsWith('+') && !line.startsWith('+++')) {
            const regex = new RegExp(pattern);
            if (regex.test(line)) {
              found = true;
              if (!foundIn.includes(fileName)) {
                foundIn.push(fileName);
              }
            }
          }
        }
      }

      if (!found) {
        results.passed = false;
        results.failures.push(`Required pattern not found: "${pattern}"`);
        results.checks[`required_pattern:${pattern}`] = false;
        console.log(`  ✗ Pattern "${pattern}" not found`);
      } else {
        results.checks[`required_pattern:${pattern}`] = true;
        console.log(`  ✓ Pattern "${pattern}" found in ${foundIn.join(', ')}`);
      }
    }
  }

  // Run custom scripts if specified
  if (checks.custom_scripts) {
    console.log('\nRunning custom scripts...');

    for (const script of checks.custom_scripts) {
      const scriptName = script.name || script.script;
      const scriptCommand = script.script;
      const workingDir = script.cwd || outputDir;

      console.log(`  Running: ${scriptName}`);

      try {
        const result = spawnSync('bash', ['-c', scriptCommand], {
          cwd: workingDir,
          encoding: 'utf8',
          stdio: 'pipe',
          timeout: script.timeout || 30000
        });

        const passed = result.status === 0;
        results.checks[`custom_script:${scriptName}`] = passed;

        if (passed) {
          console.log(`    ✓ ${scriptName} passed`);
        } else {
          results.passed = false;
          results.failures.push(`Custom script failed: ${scriptName}`);
          console.log(`    ✗ ${scriptName} failed`);
          if (result.stderr) {
            const lines = result.stderr.split('\n').slice(0, 5);
            lines.forEach(line => console.log(`      ${line}`));
          }
        }
      } catch (error) {
        results.passed = false;
        results.failures.push(`Custom script error: ${scriptName} - ${error.message}`);
        results.checks[`custom_script:${scriptName}`] = false;
        console.log(`    ✗ ${scriptName} error: ${error.message}`);
      }
    }
  }

  return results;
}

/**
 * Run optional static evaluation criteria
 */
async function runOptionalChecks(outputDir, testDef) {
  console.log('\n=== Running Optional Checks ===\n');

  const results = {
    warnings: [],
    checks: {}
  };

  const checks = testDef.optional_checks || {};

  // Optional file checks
  if (checks.files_exist) {
    console.log('Checking optional files...');
    for (const file of checks.files_exist) {
      const filePath = join(outputDir, file);
      const exists = existsSync(filePath);
      results.checks[`optional_file:${file}`] = exists;

      if (!exists) {
        results.warnings.push(`Optional file does not exist: ${file}`);
        console.log(`  ⚠ ${file} (missing but optional)`);
      } else {
        console.log(`  ✓ ${file}`);
      }
    }
  }

  return results;
}

/**
 * Check PR quality if PR was opened
 */
async function checkPRQuality(outputDir, testDef) {
  console.log('\n=== Checking PR Quality ===\n');

  const results = {
    pr_opened: false,
    pr_url: null,
    pr_quality: {},
    failures: []
  };

  // Try to detect PR from git branch
  try {
    // Get the branch name from the output directory
    // The test was run in an isolated branch
    const agentInfoPath = join(outputDir, 'agent-info.json');
    let branchName = null;

    if (existsSync(agentInfoPath)) {
      const agentInfo = JSON.parse(readFileSync(agentInfoPath, 'utf8'));
      branchName = agentInfo.branch;
    }

    if (!branchName) {
      console.log('  ℹ Could not determine branch name');
      console.log('  ℹ No PR opened (no penalty)');
      return results;
    }

    // Check if a PR exists for this branch using gh CLI
    try {
      const prListResult = spawnSync('gh', ['pr', 'list', '--head', branchName, '--json', 'url,number,state'], {
        cwd: PROJECT_ROOT,
        encoding: 'utf8',
        stdio: 'pipe'
      });

      if (prListResult.status === 0 && prListResult.stdout.trim()) {
        const prs = JSON.parse(prListResult.stdout);
        if (prs && prs.length > 0) {
          const pr = prs[0];
          results.pr_opened = true;
          results.pr_url = pr.url;
          console.log(`  ✓ PR detected: ${pr.url}`);

          // Run quality checks if PR was opened
          const checks = testDef.pr_quality_checks || {};

          // Check CI/CD status
          if (checks.checks_pass !== false) {
            console.log('\n  Checking CI/CD status...');
            const checksResult = spawnSync('gh', ['pr', 'checks', pr.number.toString()], {
              cwd: PROJECT_ROOT,
              encoding: 'utf8',
              stdio: 'pipe'
            });

            if (checksResult.status === 0) {
              const output = checksResult.stdout;
              // Parse check results - if any failed, mark as failure
              const failedChecks = output.split('\n').filter(line => line.includes('fail'));
              if (failedChecks.length > 0) {
                results.pr_quality.checks_pass = false;
                results.failures.push('PR has failing checks');
                console.log('    ✗ Some checks failed');
                failedChecks.forEach(check => console.log(`      - ${check.trim()}`));
              } else {
                results.pr_quality.checks_pass = true;
                console.log('    ✓ All checks passing');
              }
            }
          }

          // Check for preview link in PR body
          if (checks.has_preview_link !== false) {
            console.log('\n  Checking for preview link...');
            const prViewResult = spawnSync('gh', ['pr', 'view', pr.number.toString(), '--json', 'body'], {
              cwd: PROJECT_ROOT,
              encoding: 'utf8',
              stdio: 'pipe'
            });

            if (prViewResult.status === 0) {
              const prData = JSON.parse(prViewResult.stdout);
              const body = prData.body || '';

              // Look for preview links (*.aem.page URLs)
              const previewLinkMatch = body.match(/https?:\/\/[a-zA-Z0-9-]+--[a-zA-Z0-9-]+--[a-zA-Z0-9-]+\.aem\.(page|live)/);
              if (previewLinkMatch) {
                results.pr_quality.has_preview_link = true;
                results.pr_quality.preview_url = previewLinkMatch[0];
                console.log(`    ✓ Preview link found: ${previewLinkMatch[0]}`);
              } else {
                results.pr_quality.has_preview_link = false;
                results.failures.push('PR missing preview link');
                console.log('    ✗ No preview link found');
              }
            }
          }

          // Check if preview link is valid (optional, can be slow)
          if (checks.preview_valid !== false && results.pr_quality.preview_url) {
            console.log('\n  Checking preview link validity...');
            try {
              const curlResult = spawnSync('curl', ['-I', '-s', '-o', '/dev/null', '-w', '%{http_code}', results.pr_quality.preview_url], {
                encoding: 'utf8',
                stdio: 'pipe',
                timeout: 10000
              });

              const statusCode = parseInt(curlResult.stdout.trim());
              if (statusCode >= 200 && statusCode < 400) {
                results.pr_quality.preview_valid = true;
                console.log(`    ✓ Preview link returns ${statusCode}`);
              } else {
                results.pr_quality.preview_valid = false;
                results.failures.push(`Preview link returns ${statusCode}`);
                console.log(`    ✗ Preview link returns ${statusCode}`);
              }
            } catch (error) {
              results.pr_quality.preview_valid = false;
              results.failures.push('Preview link check timed out or failed');
              console.log('    ✗ Could not validate preview link');
            }
          }

        } else {
          console.log('  ℹ No PR found for this branch');
          console.log('  ℹ No PR opened (no penalty)');
        }
      }
    } catch (error) {
      console.log('  ℹ Error checking for PR (gh CLI might not be available)');
      console.log(`  ℹ ${error.message}`);
    }

  } catch (error) {
    console.log('  ℹ Error in PR quality check');
    console.log(`  ℹ ${error.message}`);
  }

  return results;
}

/**
 * Run dynamic criteria evaluation with LLM
 */
async function runNonDeterministicEvaluation(outputDir, testDef, evalAgent) {
  console.log('\n=== Running Non-Deterministic Criteria Evaluation ===\n');
  console.log(`Using agent: ${evalAgent}\n`);

  const results = {
    by_priority: {
      high: {},
      medium: {},
      low: {}
    },
    overall_notes: []
  };

  const criteria = testDef.dynamic_criteria || testDef.flexible_criteria || []; // Support old name

  if (criteria.length === 0) {
    console.log('  ℹ No dynamic criteria defined');
    return results;
  }

  console.log(`Evaluating ${criteria.length} criteria...\n`);

  try {
    // Build evaluation prompt
    const prompt = buildEvaluationPrompt(outputDir, testDef, criteria);

    // Save prompt to file for reference
    const promptPath = join(outputDir, 'evaluation-prompt.txt');
    writeFileSync(promptPath, prompt);
    console.log(`  ℹ Saved evaluation prompt: ${promptPath}`);

    // Run evaluation agent
    console.log(`  ℹ Running ${evalAgent} for evaluation...\n`);

    const evalResultPath = join(outputDir, 'evaluation-response.json');
    const evalPromptFile = join(outputDir, 'eval-task.txt');

    // Create a structured task for the agent
    const agentTask = `${prompt}

IMPORTANT: You must respond with ONLY a valid JSON object matching this schema:
{
  "by_priority": {
    "high": {
      "<criterion_name>": {
        "strengths": ["string"],
        "issues": ["string"],
        "notes": ["string"]
      }
    },
    "medium": { /* same structure */ },
    "low": { /* same structure */ }
  },
  "overall_notes": ["string"]
}

Do not include any other text before or after the JSON. The response must be valid JSON.`;

    writeFileSync(evalPromptFile, agentTask);

    // Invoke the evaluation agent
    console.log(`  Invoking ${evalAgent} for evaluation...\n`);

    try {
      let agentCommand;
      let agentArgs = [];

      switch (evalAgent) {
        case 'claude-code':
          agentCommand = 'claude';
          agentArgs = [
            '--permission-mode', 'bypassPermissions',
            '--output-format', 'json',
            '--print', agentTask
          ];
          break;
        case 'cursor-cli':
          agentCommand = 'cursor-agent';
          agentArgs = ['--force', agentTask];
          break;
        case 'codex-cli':
          agentCommand = 'codex';
          agentArgs = ['exec', '--dangerously-bypass-approvals-and-sandbox', '--json', agentTask];
          break;
        default:
          throw new Error(`Unknown eval agent: ${evalAgent}`);
      }

      console.log(`  Running: ${agentCommand} ${agentArgs.slice(0, 3).join(' ')}...`);

      const evalResult = spawnSync(agentCommand, agentArgs, {
        cwd: outputDir,
        encoding: 'utf8',
        stdio: 'pipe',
        timeout: 300000 // 5 minutes
      });

      if (evalResult.error) {
        throw new Error(`Failed to run ${evalAgent}: ${evalResult.error.message}`);
      }

      // Save raw output
      writeFileSync(join(outputDir, 'eval-agent-output.txt'), evalResult.stdout + '\n\n' + evalResult.stderr);

      // Try to parse JSON from output
      let evaluationData;
      try {
        // For claude-code with --output-format json, response is in stdout
        // For other agents, might need to extract from different places
        const output = evalResult.stdout.trim();

        // First, try to parse as JSON (might be wrapped format from claude-code)
        let parsedOutput;
        try {
          parsedOutput = JSON.parse(output);
        } catch (e) {
          // If that fails, try to extract JSON from text
          const jsonMatch = output.match(/\{[\s\S]*\}/);
          if (jsonMatch) {
            parsedOutput = JSON.parse(jsonMatch[0]);
          } else {
            throw new Error('Could not find JSON in output');
          }
        }

        // Check if this is claude-code wrapped format (has 'result' field)
        if (parsedOutput.result && typeof parsedOutput.result === 'string') {
          // Extract JSON from markdown code block in result
          const codeBlockMatch = parsedOutput.result.match(/```json\s*([\s\S]*?)\s*```/);
          if (codeBlockMatch) {
            evaluationData = JSON.parse(codeBlockMatch[1]);
          } else {
            // Try to find JSON directly in result
            const jsonMatch = parsedOutput.result.match(/\{[\s\S]*\}/);
            if (jsonMatch) {
              evaluationData = JSON.parse(jsonMatch[0]);
            } else {
              evaluationData = parsedOutput; // Use the parsed output as-is
            }
          }
        } else {
          // Direct format (cursor-cli, codex-cli, etc.)
          evaluationData = parsedOutput;
        }

        console.log('  ✓ Received evaluation from agent\n');

        // Validate structure
        if (evaluationData.by_priority) {
          results.by_priority = evaluationData.by_priority;
        }
        if (evaluationData.overall_notes) {
          results.overall_notes = evaluationData.overall_notes;
        }

        // Save parsed evaluation
        writeFileSync(join(outputDir, 'eval-agent-response.json'), JSON.stringify(evaluationData, null, 2));

      } catch (parseError) {
        console.log(`  ⚠ Could not parse agent response as JSON: ${parseError.message}`);
        console.log('  Creating placeholder results instead\n');

        // Fall back to placeholder results
        for (const criterion of criteria) {
          const priority = criterion.priority || 'medium';

          if (!results.by_priority[priority]) {
            results.by_priority[priority] = {};
          }

          results.by_priority[priority][criterion.name] = {
            strengths: ['[Could not parse agent response]'],
            issues: [],
            notes: [`Agent output saved to eval-agent-output.txt`, `Criterion: ${criterion.description}`]
          };
        }

        results.overall_notes.push(`Note: Agent response could not be parsed. See eval-agent-output.txt for raw output.`);
      }

    } catch (error) {
      console.log(`  ✗ Error running eval agent: ${error.message}\n`);

      // Create placeholder results
      for (const criterion of criteria) {
        const priority = criterion.priority || 'medium';

        if (!results.by_priority[priority]) {
          results.by_priority[priority] = {};
        }

        results.by_priority[priority][criterion.name] = {
          strengths: [],
          issues: [`Evaluation agent failed: ${error.message}`],
          notes: [`Criterion: ${criterion.description}`]
        };
      }

      results.overall_notes.push(`Error: Could not run evaluation agent - ${error.message}`);
    }

  } catch (error) {
    console.log(`  ✗ Error in dynamic evaluation: ${error.message}`);
    results.overall_notes.push(`Error during evaluation: ${error.message}`);
  }

  return results;
}

/**
 * Build evaluation prompt for LLM
 */
function buildEvaluationPrompt(outputDir, testDef, criteria) {
  let prompt = `# Agent Skills Test Evaluation

You are evaluating the results of an agent skills test. Your task is to assess the agent's performance based on the dynamic criteria defined for this test.

## Test Information

**Test Name:** ${testDef.name}
**Description:** ${testDef.description || 'N/A'}
**Task:** ${testDef.task}

## Evaluation Criteria

You will evaluate the following criteria, organized by priority:

`;

  // Group criteria by priority
  const byPriority = { high: [], medium: [], low: [] };
  for (const criterion of criteria) {
    const priority = criterion.priority || 'medium';
    byPriority[priority].push(criterion);
  }

  for (const priority of ['high', 'medium', 'low']) {
    if (byPriority[priority].length > 0) {
      prompt += `### ${priority.toUpperCase()} Priority\n\n`;
      for (const criterion of byPriority[priority]) {
        prompt += `- **${criterion.name}**: ${criterion.description}\n`;
      }
      prompt += '\n';
    }
  }

  prompt += `## Artifacts to Review

The following artifacts are available in the output directory:

`;

  // List available artifacts
  try {
    const files = readdirSync(outputDir);
    const relevantFiles = files.filter(f => {
      return f.endsWith('.js') || f.endsWith('.css') || f.endsWith('.json') ||
             f.endsWith('.md') || f.endsWith('.diff') || f === 'agent-info.json';
    });

    for (const file of relevantFiles) {
      prompt += `- ${file}\n`;
    }
  } catch (error) {
    prompt += '- (Error listing files)\n';
  }

  prompt += `
## Your Task

For each criterion, provide:

1. **Strengths**: What went well? What did the agent do correctly?
2. **Issues**: What didn't go well? What could be improved?
3. **Notes**: Additional observations or context

Also provide **Overall Notes** with general observations about the agent's performance.

## Important Guidelines

- Focus on qualitative assessment, not scores
- Consider the specific task and context
- Acknowledge that there may be multiple valid approaches
- Note both successes and areas for improvement
- Be constructive and specific in your feedback

## Output Format

Organize your findings by priority (high/medium/low), with each criterion having:
- strengths: array of strings
- issues: array of strings
- notes: array of strings

Plus overall_notes as an array of general observations.
`;

  return prompt;
}

/**
 * Generate evaluation outputs
 */
function generateOutputs(outputDir, evaluationResults) {
  console.log('\n=== Generating Evaluation Outputs ===\n');

  const timestamp = new Date().toISOString();

  // Generate JSON output
  const jsonOutput = {
    ...evaluationResults,
    timestamp,
    version: '1.0.0'
  };

  const jsonPath = join(outputDir, 'evaluation-results.json');
  writeFileSync(jsonPath, JSON.stringify(jsonOutput, null, 2));
  console.log(`  ✓ Saved JSON: ${jsonPath}`);

  // Generate Markdown report
  const mdContent = generateMarkdownReport(evaluationResults);
  const mdPath = join(outputDir, 'evaluation-report.md');
  writeFileSync(mdPath, mdContent);
  console.log(`  ✓ Saved report: ${mdPath}`);

  return { jsonPath, mdPath };
}

/**
 * Generate markdown report
 */
function generateMarkdownReport(results) {
  let md = '# Evaluation Report\n\n';

  md += `**Timestamp:** ${new Date().toISOString()}\n\n`;
  md += `**Test:** ${results.task_name || 'Unknown'}\n\n`;
  md += `**Agent (tested):** ${results.agent || 'Unknown'}\n\n`;
  md += `**Evaluator:** ${results.evaluator || 'Unknown'}\n\n`;

  md += '## Deterministic Results\n\n';
  md += `**Status:** ${results.static_results?.passed ? '✅ PASSED' : '❌ FAILED'}\n\n`;

  // Show all checks that were run
  if (results.static_results?.checks) {
    md += '### Checks Run\n\n';
    const checks = results.static_results.checks;
    for (const [checkName, passed] of Object.entries(checks)) {
      const icon = passed ? '✅' : '❌';
      md += `- ${icon} ${checkName}\n`;
    }
    md += '\n';
  }

  if (results.static_results?.failures?.length > 0) {
    md += '### Failures\n\n';
    for (const failure of results.static_results.failures) {
      md += `- ❌ ${failure}\n`;
    }
    md += '\n';
  }

  // Show optional checks that were run
  if (results.optional_results?.checks && Object.keys(results.optional_results.checks).length > 0) {
    md += '### Optional Checks\n\n';
    const checks = results.optional_results.checks;
    for (const [checkName, passed] of Object.entries(checks)) {
      const icon = passed ? '✅' : '⚠️';
      md += `- ${icon} ${checkName}\n`;
    }
    md += '\n';
  }

  if (results.optional_results?.warnings?.length > 0) {
    md += '### Warnings\n\n';
    for (const warning of results.optional_results.warnings) {
      md += `- ⚠️ ${warning}\n`;
    }
    md += '\n';
  }

  // Show PR results if PR was opened
  if (results.pr_results?.pr_opened) {
    md += '### Pull Request\n\n';
    md += `**URL:** ${results.pr_results.pr_url}\n\n`;
    if (results.pr_results.pr_quality && Object.keys(results.pr_results.pr_quality).length > 0) {
      md += '**Quality Checks:**\n\n';
      for (const [check, result] of Object.entries(results.pr_results.pr_quality)) {
        if (typeof result === 'boolean') {
          const icon = result ? '✅' : '❌';
          md += `- ${icon} ${check}\n`;
        } else if (check === 'preview_url') {
          md += `- Preview URL: ${result}\n`;
        }
      }
      md += '\n';
    }
    if (results.pr_results.failures?.length > 0) {
      md += '**PR Failures:**\n\n';
      for (const failure of results.pr_results.failures) {
        md += `- ❌ ${failure}\n`;
      }
      md += '\n';
    }
  }

  md += '## Non-Deterministic Criteria Assessment\n\n';

  if (results.dynamic_assessment && results.dynamic_assessment.by_priority) {
    const assessment = results.dynamic_assessment;

    for (const priority of ['high', 'medium', 'low']) {
      if (assessment.by_priority[priority] && Object.keys(assessment.by_priority[priority]).length > 0) {
        md += `### ${priority.toUpperCase()} Priority\n\n`;

        for (const [criterionName, criterionResults] of Object.entries(assessment.by_priority[priority])) {
          md += `#### ${criterionName}\n\n`;

          if (criterionResults.strengths && criterionResults.strengths.length > 0) {
            md += '**Strengths:**\n\n';
            criterionResults.strengths.forEach(s => md += `- ✅ ${s}\n`);
            md += '\n';
          }

          if (criterionResults.issues && criterionResults.issues.length > 0) {
            md += '**Issues:**\n\n';
            criterionResults.issues.forEach(i => md += `- ⚠️ ${i}\n`);
            md += '\n';
          }

          if (criterionResults.notes && criterionResults.notes.length > 0) {
            md += '**Notes:**\n\n';
            criterionResults.notes.forEach(n => md += `- ${n}\n`);
            md += '\n';
          }
        }
      }
    }

    if (assessment.overall_notes && assessment.overall_notes.length > 0) {
      md += '### Overall Notes\n\n';
      assessment.overall_notes.forEach(note => md += `- ${note}\n`);
      md += '\n';
    }

  } else {
    md += '_(Not evaluated)_\n\n';
  }

  return md;
}

/**
 * Evaluate a single agent's results
 */
async function evaluateSingleAgent(agentDir, testDef, options) {
  const pathParts = agentDir.split('/');
  const agent = pathParts[pathParts.length - 1];
  const testPath = pathParts.slice(pathParts.indexOf('test-results') + 1, -2).join('/');

  console.log(`\n${'='.repeat(60)}`);
  console.log(`Evaluating: ${agent}`);
  console.log('='.repeat(60));

  // Run evaluations
  const deterministic = await runDeterministicChecks(agentDir, testDef);
  const optional = await runOptionalChecks(agentDir, testDef);
  const prQuality = await checkPRQuality(agentDir, testDef);

  let nonDeterministic = null;
  if (!options.skipNonDeterministic) {
    nonDeterministic = await runNonDeterministicEvaluation(agentDir, testDef, options.evalAgent);
  } else {
    console.log('\n=== Skipping Non-Deterministic Evaluation ===\n');
  }

  // Combine results
  const evaluationResults = {
    task_name: testPath,
    test_definition_name: testDef.name,
    agent,
    evaluator: options.evalAgent,
    static_results: deterministic,
    optional_results: optional,
    pr_results: prQuality,
    dynamic_assessment: nonDeterministic
  };

  // Generate outputs
  const { jsonPath, mdPath } = generateOutputs(agentDir, evaluationResults);

  console.log(`\nStatus: ${deterministic.passed ? '✅ PASSED' : '❌ FAILED'}`);
  console.log(`Results saved to:`);
  console.log(`  - ${jsonPath}`);
  console.log(`  - ${mdPath}`);

  return {
    agent,
    passed: deterministic.passed,
    results: evaluationResults
  };
}

/**
 * Main execution
 */
async function main() {
  try {
    const options = parseArgs();

    console.log('='.repeat(60));
    console.log('Agent Skills Test Evaluation');
    console.log('='.repeat(60));
    console.log(`\nOutput Directory: ${options.outputDir}`);
    console.log(`Eval Agent: ${options.evalAgent}`);
    console.log(`Skip Non-Deterministic: ${options.skipNonDeterministic}`);

    // Detect path type first
    const pathType = detectPathType(options.outputDir);
    console.log(`\nPath Type: ${pathType}`);

    let agentDirsToEvaluate = [];

    if (pathType === 'timestamp') {
      // evaluations/{timestamp} - evaluate all tasks and agents
      const taskDirs = getTaskDirectories(options.outputDir);
      console.log(`\nFound ${taskDirs.length} task(s)`);

      for (const taskDir of taskDirs) {
        const agentDirs = getAgentDirectoriesForTask(taskDir);
        agentDirsToEvaluate.push(...agentDirs);
      }

    } else if (pathType === 'task') {
      // evaluations/{timestamp}/{task-name} - evaluate all agents for this task
      agentDirsToEvaluate = getAgentDirectoriesForTask(options.outputDir);

    } else if (pathType === 'agent') {
      // evaluations/{timestamp}/{task-name}/{agent} - evaluate single agent
      agentDirsToEvaluate = [options.outputDir];

    } else {
      throw new Error(`Invalid path type: ${pathType}. Expected timestamp, task, or agent directory.`);
    }

    console.log(`\nEvaluating ${agentDirsToEvaluate.length} agent result(s)...\n`);

    // Evaluate each agent directory
    const allResults = [];

    for (const agentDir of agentDirsToEvaluate) {
      try {
        // Load test definition for this agent's task
        const testDef = loadTestDefinition(agentDir);
        console.log(`\n${'='.repeat(60)}`);
        console.log(`Task: ${testDef.name}`);
        console.log(`Agent: ${basename(agentDir)}`);
        console.log('='.repeat(60));

        const result = await evaluateSingleAgent(agentDir, testDef, options);
        allResults.push(result);

      } catch (error) {
        console.error(`\n❌ Error evaluating ${agentDir}:`);
        console.error(`   ${error.message}`);
        allResults.push({
          agent: basename(agentDir),
          task: basename(dirname(agentDir)),
          passed: false,
          error: error.message
        });
      }
    }

    // Print summary
    console.log('\n' + '='.repeat(60));
    console.log('Evaluation Summary');
    console.log('='.repeat(60));

    const grouped = {};
    for (const result of allResults) {
      const taskName = result.task || 'unknown';
      if (!grouped[taskName]) {
        grouped[taskName] = [];
      }
      grouped[taskName].push(result);
    }

    for (const [taskName, results] of Object.entries(grouped)) {
      console.log(`\n${taskName}:`);
      for (const result of results) {
        const status = result.passed ? '✅ PASSED' : '❌ FAILED';
        console.log(`  ${result.agent}: ${status}`);
      }
    }

    // Exit with failure if any agent failed
    const allPassed = allResults.every(r => r.passed);
    console.log(`\n${'='.repeat(60)}`);
    console.log(`Overall: ${allPassed ? '✅ ALL PASSED' : '❌ SOME FAILED'}`);
    console.log('='.repeat(60));

    process.exit(allPassed ? 0 : 1);

  } catch (error) {
    console.error('\n❌ Error:', error.message);
    console.error(error.stack);
    process.exit(1);
  }
}

main();
